{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068a65e4-1884-439c-a87c-ea852241108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brittanysteenbergen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brittanysteenbergen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brittanysteenbergen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "import string \n",
    "import csv\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cc04bc-0e4d-4e61-a5b3-fad7cbe4cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WordPunctTokenizer() \n",
    "\n",
    "def tokenizing_nltk(doc):\n",
    "    doc_tokenized = tk.tokenize(doc)\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in doc_tokenized:\n",
    "        nltk_lemmaList.append(wordnet_lemmatizer.lemmatize(word))\n",
    "\n",
    "    filtered_sentence = []  \n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    my_stop_words = {\"I\", \"4\", \"ha\", \"c\", \")\", \").\", \"|\", \"3\", \"-\", \"(\",\"–\", \"e\", \"249\"}\n",
    "    \n",
    "    for w in nltk_lemmaList:  \n",
    "        if w not in nltk_stop_words and w not in my_stop_words:  \n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    punctuations=\"?:!.,;-()|/\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1d41b4-7439-4779-a55b-18302db78405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(file):\n",
    "    survey = open(file, 'r')\n",
    "    questions = survey.readlines()\n",
    "    questions = [line.lower() for line in questions] \n",
    "    questions = [tokenizing_nltk(doc) for doc in questions] \n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in questions]\n",
    "    \n",
    "    Lda = gensim.models.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics = 9, alpha = 0.1, id2word = dictionary, passes = 50)\n",
    "\n",
    "    top_words_per_topic = []\n",
    "    for t in range(ldamodel.num_topics):\n",
    "        top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = 3)])\n",
    "\n",
    "    pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words_\" + file.rsplit('.', maxsplit=1)[0] + \".csv\")\n",
    "    \n",
    "    return ldamodel.print_topics(num_words = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605b8edf-7ac1-4d54-a625-135118afd713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  mathematics.txt\n",
      "Topics:  [(0, '0.069*\"faculty\" + 0.069*\"department\" + 0.069*\"interaction\"'), (1, '0.012*\"comment\" + 0.012*\"advising\" + 0.012*\"experience\"'), (2, '0.081*\"lmu\" + 0.081*\"department\" + 0.043*\"graduation\"'), (3, '0.053*\"mathematics\" + 0.053*\"feel\" + 0.053*\"confidence\"'), (4, '0.051*\"experience\" + 0.051*\".\" + 0.051*\"advising\"'), (5, '0.089*\"course\" + 0.089*\"unit\" + 0.089*\"department\"'), (6, '0.117*\"math\" + 0.104*\"division\" + 0.078*\"upper\"'), (7, '0.088*\"advising\" + 0.088*\"feel\" + 0.046*\"transition\"'), (8, '0.108*\"division\" + 0.083*\"lower\" + 0.083*\"course\"')]\n",
      "File:  hhsc.txt\n",
      "Topics:  [(0, '0.080*\"please\" + 0.080*\"indicate\" + 0.080*\"school\"'), (1, '0.086*\"course\" + 0.086*\"?\" + 0.086*\"taken\"'), (2, '0.091*\"advising\" + 0.074*\"department\" + 0.074*\"science\"'), (3, '0.149*\"graduate\" + 0.101*\"school\" + 0.053*\"please\"'), (4, '0.076*\"project\" + 0.040*\"yes\" + 0.040*\"lmu\"'), (5, '0.102*\"lmu\" + 0.102*\"begin\" + 0.010*\"situation\"'), (6, '0.013*\"travel\" + 0.013*\"attending\" + 0.013*\"lmu\"'), (7, '0.094*\"health\" + 0.094*\"human\" + 0.094*\"science\"'), (8, '0.073*\"major\" + 0.049*\"original\" + 0.049*\"wa\"')]\n",
      "File:  hhsc_alum.txt\n",
      "Topics:  [(0, '0.066*\"application\" + 0.049*\"course\" + 0.049*\"division\"'), (1, '0.075*\"current\" + 0.040*\"course\" + 0.040*\"related\"'), (2, '0.050*\"professional\" + 0.050*\"society\" + 0.050*\"organization\"'), (3, '0.127*\"science\" + 0.085*\"human\" + 0.064*\"health\"'), (4, '0.066*\"minor\" + 0.045*\"hhsc\" + 0.024*\"lmu\"'), (5, '0.049*\"setting\" + 0.049*\"communication\" + 0.049*\"interpersonal\"'), (6, '0.045*\"service\" + 0.045*\"valuable\" + 0.045*\"ntls\"'), (7, '0.047*\"lmu\" + 0.047*\"graduate\" + 0.025*\"inside\"'), (8, '0.077*\"would\" + 0.059*\"ntls\" + 0.059*\"overall\"')]\n",
      "File:  elec_eng.txt\n",
      "Topics:  [(0, '0.097*\"people\" + 0.026*\"background\" + 0.021*\"activity\"'), (1, '0.046*\"ece\" + 0.031*\"location\" + 0.031*\"lmu\"'), (2, '0.047*\"dissatisfied\" + 0.047*\"’\" + 0.047*\"‘\"'), (3, '0.033*\"lmu\" + 0.033*\"ece\" + 0.032*\"remark\"'), (4, '0.067*\"lmu\" + 0.066*\"ece\" + 0.048*\"following\"'), (5, '0.064*\"ece\" + 0.049*\"experience\" + 0.043*\"community\"'), (6, '0.075*\"ece\" + 0.068*\"lmu\" + 0.031*\"would\"'), (7, '0.041*\"area\" + 0.028*\"study\" + 0.028*\"lab\"'), (8, '0.032*\"equitable\" + 0.032*\"harassment\" + 0.032*\"discrimination\"')]\n",
      "File:  physics.txt\n",
      "Topics:  [(0, '0.052*\"result\" + 0.052*\"problem\" + 0.052*\"applying\"'), (1, '0.078*\"providing\" + 0.078*\"learning\" + 0.078*\"service\"'), (2, '0.012*\"providing\" + 0.012*\"graduate\" + 0.012*\"career\"'), (3, '0.081*\"program\" + 0.042*\"experience\" + 0.042*\"student\"'), (4, '0.265*\"physic\" + 0.134*\"applied\" + 0.096*\"program\"'), (5, '0.055*\"perform\" + 0.055*\"method\" + 0.055*\"calculation\"'), (6, '0.123*\"physic\" + 0.065*\"know\" + 0.065*\"theory\"'), (7, '0.091*\"physical\" + 0.048*\"effectively\" + 0.048*\"result\"'), (8, '0.058*\"student\" + 0.058*\"graduate\" + 0.058*\"school\"')]\n",
      "File:  envir_sci.txt\n",
      "Topics:  [(0, '0.069*\"please\" + 0.069*\"program\" + 0.069*\"college\"'), (1, '0.117*\"job\" + 0.062*\"indicate\" + 0.062*\"title\"'), (2, '0.130*\"lmu\" + 0.088*\"internship\" + 0.088*\"student\"'), (3, '0.064*\"lmu\" + 0.064*\"please\" + 0.064*\"school\"'), (4, '0.016*\"description\" + 0.016*\"list\" + 0.016*\"brief\"'), (5, '0.016*\"description\" + 0.016*\"list\" + 0.016*\"brief\"'), (6, '0.106*\"school\" + 0.106*\"applying\" + 0.106*\"graduate\"'), (7, '0.062*\"course\" + 0.062*\"?\" + 0.062*\"took\"'), (8, '0.106*\"please\" + 0.106*\"job\" + 0.056*\"looking\"')]\n",
      "File:  comp_sci.txt\n",
      "Topics:  [(0, '0.108*\"identity\" + 0.073*\"feel\" + 0.049*\"aspect\"'), (1, '0.072*\"lmu\" + 0.049*\"lab\" + 0.049*\"found\"'), (2, '0.069*\"experienced\" + 0.047*\"lmu\" + 0.047*\"harassing\"'), (3, '0.071*\"encouragement\" + 0.048*\"feel\" + 0.048*\"experienced\"'), (4, '0.066*\"lmu\" + 0.035*\"someone\" + 0.035*\"encouragement\"'), (5, '0.064*\"way\" + 0.064*\"following\" + 0.064*\"mark\"'), (6, '0.051*\"feel\" + 0.035*\"lmucs\" + 0.035*\"feeling\"'), (7, '0.038*\"someone\" + 0.038*\"generally\" + 0.038*\"people\"'), (8, '0.057*\"major\" + 0.030*\"online\" + 0.030*\"career\"')]\n",
      "File:  mech_eng.txt\n",
      "Topics:  [(0, '0.008*\"graduate\" + 0.008*\"school\" + 0.008*\"opportunity\"'), (1, '0.052*\"graduate\" + 0.052*\"school\" + 0.052*\"program\"'), (2, '0.057*\"engineering\" + 0.039*\"appropriate\" + 0.039*\"using\"'), (3, '0.069*\"name\" + 0.047*\"program\" + 0.047*\"would\"'), (4, '0.031*\"provide\" + 0.031*\"effectively\" + 0.031*\"meet\"'), (5, '0.031*\"providing\" + 0.031*\"learning\" + 0.031*\"professional\"'), (6, '0.067*\"engineering\" + 0.024*\"apply\" + 0.024*\"opportunity\"'), (7, '0.050*\"student\" + 0.050*\"advising\" + 0.050*\"option\"'), (8, '0.089*\"program\" + 0.089*\"mechanical\" + 0.089*\"engineering\"')]\n",
      "File:  biology_chemistry.txt\n",
      "Topics:  [(0, '0.066*\"participate\" + 0.066*\"scientific\" + 0.066*\"opportunity\"'), (1, '0.036*\"participate\" + 0.036*\"time\" + 0.036*\"second\"'), (2, '0.065*\"biology\" + 0.065*\"department\" + 0.065*\"would\"'), (3, '0.096*\"feel\" + 0.051*\"future\" + 0.051*\"career\"'), (4, '0.031*\"future\" + 0.031*\"include\" + 0.031*\"name\"'), (5, '0.062*\"one\" + 0.062*\"indicate\" + 0.062*\"please\"'), (6, '0.053*\"scientific\" + 0.053*\"developed\" + 0.053*\"information\"'), (7, '0.043*\"future\" + 0.043*\"undergraduate\" + 0.043*\"member\"'), (8, '0.076*\"biology\" + 0.040*\"student\" + 0.040*\"wa\"')]\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        print('File: ', file)\n",
    "        print('Topics: ', lda(file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
